{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8529bec2-667e-4408-b86a-6afb74773e6a",
   "metadata": {},
   "source": [
    "Module - CIS7017 Dissertation\n",
    "Student ID - #20275320"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b6650214d0ad84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86eec08bab5bb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all relevant libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('C:/dataset/US_Accidents.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab7024128d395b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c18ecca3d6ae9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for rows where the State column is 'UT' for Utah\n",
    "for column in ['State']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(data[column].value_counts())\n",
    "    \n",
    "utah_data = data[data['State'] == 'UT']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a502514202324080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Altitude data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a40ed7d64dfc6c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to get elevations for a list of latitudes and longitudes\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Validate latitudes and longitudes\n",
    "    valid_latitudes = [str(lat) for lat in latitudes if -90 <= lat <= 90]\n",
    "    valid_longitudes = [str(lon) for lon in longitudes if -180 <= lon <= 180]\n",
    "    \n",
    "    # Ensure we have the same number of valid latitudes and longitudes\n",
    "    if len(valid_latitudes) != len(valid_longitudes) or not valid_latitudes:\n",
    "        return [None] * len(latitudes)  # Return None for invalid pairs\n",
    "    \n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(valid_latitudes)}&longitude={','.join(valid_longitudes)}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # an exception for HTTP error codes\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Splitting the DataFrame into chunks of 100 rows to comply with the API's limitation\n",
    "chunk_size = 100\n",
    "altitude_list = []\n",
    "\n",
    "# Wrap the range function with tqdm to see the progress\n",
    "for start in tqdm(range(0, utah_data.shape[0], chunk_size), desc='Fetching Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = utah_data.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    altitude_list.extend(elevations)\n",
    "\n",
    "# Add the altitude information to the DataFrame\n",
    "utah_data['Altitude'] = altitude_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a1fb08bba2ef9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run again for failed API requests. \n",
    "# TODO:: Merge both snippets\n",
    "\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Construct the API URL with the given latitudes and longitudes\n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(map(str, latitudes))}&longitude={','.join(map(str, longitudes))}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an error for bad responses\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Filter the DataFrame to rows where Altitude is missing (NaN)\n",
    "missing_altitude_df = utah_data[pd.isna(utah_data['Altitude'])]\n",
    "\n",
    "# Initialize an empty list to store the fetched altitudes\n",
    "fetched_altitudes = []\n",
    "\n",
    "for start in tqdm(range(0, missing_altitude_df.shape[0], chunk_size), desc='Filling Missing Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = missing_altitude_df.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    fetched_altitudes.extend(elevations)\n",
    "\n",
    "# Update the original DataFrame with the newly fetched altitudes\n",
    "for (index, altitude), (_, row) in zip(enumerate(fetched_altitudes), missing_altitude_df.iterrows()):\n",
    "    if altitude is not None:  # Only update if the API call was successful\n",
    "        utah_data.at[row.name, 'Altitude'] = altitude\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae72c1a7a0a0e75d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the draft dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b702c89b5d29bc15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877d2f04d5749c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO::Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c8c54e34d9c2fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb4c3fc3cc7bb424"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "utah_data = pd.read_csv('utah_traffic_accidents.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c2322c5c03d301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utah_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e564a6511dd1bd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Check for missing values \\n\")\n",
    "print(utah_data.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49fbd44b7636735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get value counts for a column\n",
    "for column in ['Precipitation(in)']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(utah_data[column].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf70764501b776cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping columns with less percentage of data and unnecessary columns\n",
    "\n",
    "missing = pd.DataFrame(utah_data.isnull().sum()).reset_index()\n",
    "missing.columns = ['Feature', 'Missing_Percent(%)']\n",
    "missing['Missing_Percent(%)'] = missing['Missing_Percent(%)'].apply(lambda x: x / utah_data.shape[0] * 100)\n",
    "missing.loc[missing['Missing_Percent(%)']>0,:]\n",
    "\n",
    "utah_data = utah_data.drop(columns=['Source', 'End_Lat','End_Lng','Wind_Chill(F)', 'Description', 'Street', 'County', 'Zipcode', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Amenity', 'Bump', 'Give_Way', 'No_Exit', 'Roundabout', 'Traffic_Calming', 'Turning_Loop'],axis=1)\n",
    "utah_data.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b84d00c23188ee27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove data points with missing values (for insignificant amounts)\n",
    "utah_data = utah_data.dropna(subset=['Nautical_Twilight', 'Precipitation(in)'])\n",
    "utah_data=utah_data.dropna(axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e5a31cf14086b45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "utah_data = utah_data.rename(columns={'Start_Lat': 'Geo_lat', 'Start_Lng': 'Geo_lng', 'Distance(mi)': 'Distance', 'Temperature(F)': 'Temperature',\n",
    "                                      'Humidity(%)': 'Humidity', 'Pressure(in)': 'Pressure', 'Visibility(mi)': 'Visibility', 'Wind_Speed(mph)': 'Wind_Speed',\n",
    "                                      'Precipitation(in)': 'Precipitation'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7189c7816cedb59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utah_data = utah_data.drop(columns=['State'],axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe5ba2ce13501f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop NA columns\n",
    "# nan_columns = utah_data.columns[utah_data.isnull().any()].tolist()\n",
    "# if nan_columns:\n",
    "#     raise ValueError(f\"NaN found in columns: {nan_columns}\")\n",
    "utah_data = utah_data.dropna()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f874fa5c367958"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate 'Time_Duration' in seconds\n",
    "utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time'])\n",
    "utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time'])\n",
    "utah_data['Time_Duration'] = (utah_data['End_Time'] - utah_data['Start_Time']).dt.total_seconds()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18ee206fd53f2d3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fixing fractual seconds in Time columns.\n",
    "\n",
    "try:\n",
    "    utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time']).dt.floor('S')\n",
    "    utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time']).dt.floor('S')\n",
    "except Exception as e:\n",
    "    print(\"Error converting dates:\", e)\n",
    "    utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time'], errors='coerce').dt.floor('S')\n",
    "    utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time'], errors='coerce').dt.floor('S')\n",
    "    problematic_starts = utah_data[utah_data['Start_Time'].isna()]\n",
    "    problematic_ends = utah_data[utah_data['End_Time'].isna()]\n",
    "    if not problematic_starts.empty or not problematic_ends.empty:\n",
    "        print(\"Problematic Start Times:\", problematic_starts)\n",
    "        print(\"Problematic End Times:\", problematic_ends)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c0fd77fd4f5ef7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Clean Up Categorical Features\n",
    "\n",
    "# Simplify wind direction\n",
    "print(\"Wind Direction: \", utah_data['Wind_Direction'].unique())\n",
    "utah_data.loc[utah_data['Wind_Direction']=='Calm','Wind_Direction'] = 'CALM'\n",
    "utah_data.loc[(utah_data['Wind_Direction']=='West')|(utah_data['Wind_Direction']=='WSW')|(utah_data['Wind_Direction']=='WNW'),'Wind_Direction'] = 'W'\n",
    "utah_data.loc[(utah_data['Wind_Direction']=='South')|(utah_data['Wind_Direction']=='SSW')|(utah_data['Wind_Direction']=='SSE'),'Wind_Direction'] = 'S'\n",
    "utah_data.loc[(utah_data['Wind_Direction']=='North')|(utah_data['Wind_Direction']=='NNW')|(utah_data['Wind_Direction']=='NNE'),'Wind_Direction'] = 'N'\n",
    "utah_data.loc[(utah_data['Wind_Direction']=='East')|(utah_data['Wind_Direction']=='ESE')|(utah_data['Wind_Direction']=='ENE'),'Wind_Direction'] = 'E'\n",
    "utah_data.loc[utah_data['Wind_Direction']=='Variable','Wind_Direction'] = 'VAR'\n",
    "print(\"Wind Direction after simplification: \", utah_data['Wind_Direction'].unique())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfa070f11660f702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Re-saving the cleaned dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b54ae409f236b7e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "666b959a5b15ba91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('utah_traffic_accidents.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "911e5bede8efc89d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame with a 'Date' column, 'Geo_lat', and 'Geo_lng'\n",
    "# First, ensure your Date column is in datetime format if not already\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "\n",
    "# Grouping by Date and geographic coordinates\n",
    "df = df.groupby(['Start_Time', 'Geo_lat', 'Geo_lng']).size().reset_index(name='Count')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc203e7e7a4f3b60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare Date field\n",
    "# df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "# df['Date'] = df['Start_Time'].dt.strftime('%Y-%m-%d')  # Create a temporary 'Date' column\n",
    "# df['Snowfall'] = pd.NA  # Initialize the Snowfall column"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57daf34463ad2d64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('df_with_snowdata.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00ee8e9bbc2a416"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "df = pd.read_csv('df_with_snowdata.csv')\n",
    "\n",
    "def fetch_snowfall(latitude, longitude, date):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": date,\n",
    "        \"end_date\": date,\n",
    "        \"daily\": \"snowfall_sum\",\n",
    "        \"timezone\": \"America/Denver\"\n",
    "    }\n",
    "    response = requests.get(url, params=params, timeout=50)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'daily' in data and 'snowfall_sum' in data['daily'] and data['daily']['snowfall_sum']:\n",
    "            return data['daily']['snowfall_sum'][0]\n",
    "    return None\n",
    "\n",
    "def update_snowfall(df):\n",
    "    count_calls = 0\n",
    "    hourly_limit = 10000\n",
    "    batch_limit = 600  # Maximum calls per batch\n",
    "    batch_time = 60   # Sleep time in seconds after a batch\n",
    "\n",
    "    # Filter DataFrame to only include rows where 'Snowfall' is NA\n",
    "    filtered_df = df[df['Snowfall'].isna()]\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    pbar = tqdm(total=filtered_df.shape[0], desc=\"Processing snowfall data\")\n",
    "\n",
    "    for date, group in filtered_df.groupby('Date'):\n",
    "        for idx, row in group.iterrows():\n",
    "            if count_calls >= batch_limit:\n",
    "                print(\"Reached 600 calls, waiting for 60 seconds...\")\n",
    "                time.sleep(batch_time)  # Wait for 60 seconds\n",
    "                count_calls = 0  # Reset count after waiting\n",
    "\n",
    "            if count_calls >= hourly_limit:\n",
    "                print(\"Reached hourly limit, stopping...\")\n",
    "                pbar.close()  # Ensure to close the progress bar properly\n",
    "                return  # Stop processing if hourly limit is reached\n",
    "\n",
    "            snowfall = fetch_snowfall(row['Geo_lat'], row['Geo_lng'], date)\n",
    "            if snowfall is not None:\n",
    "                df.at[idx, 'Snowfall'] = snowfall\n",
    "                count_calls += 1\n",
    "\n",
    "            pbar.update(1)  # Updating the progress \n",
    "\n",
    "    pbar.close()  # Close the progress bar\n",
    "\n",
    "update_snowfall(df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56718b7e79e87c16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df['Snowfall'].isnull().values.sum()\n",
    "# df['Snowfall'].value_counts().sum()\n",
    "# df['Snowfall'].isna().sum()\n",
    "\n",
    "# Re-saving the cleaned dataset to a CSV file\n",
    "# df.to_csv('df_with_snowdata.csv', index=False)\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdfeda50eea287fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To populate the remaining NA values in the 'Snowfall' column by using the nearest known values \n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('df_with_snowdata.csv')\n",
    "\n",
    "# Separate rows with known and unknown snowfall values\n",
    "known_snowfall = df.dropna(subset=['Snowfall'])\n",
    "na_snowfall = df[df['Snowfall'].isna()]\n",
    "\n",
    "# Convert DataFrames to GeoDataFrames\n",
    "known_snowfall['geometry'] = known_snowfall.apply(lambda row: Point(row['Geo_lng'], row['Geo_lat']), axis=1)\n",
    "na_snowfall['geometry'] = na_snowfall.apply(lambda row: Point(row['Geo_lng'], row['Geo_lat']), axis=1)\n",
    "gdf_known = gpd.GeoDataFrame(known_snowfall, geometry='geometry')\n",
    "gdf_na = gpd.GeoDataFrame(na_snowfall, geometry='geometry')\n",
    "\n",
    "# Create spatial index\n",
    "tree = BallTree(gdf_known[['Geo_lat', 'Geo_lng']], metric='haversine')\n",
    "\n",
    "# Function to find the nearest point with a known snowfall value\n",
    "def fill_snowfall(row):\n",
    "    # Find the index of the nearest point with known snowfall, within the same date\n",
    "    distance, index = tree.query([[np.radians(row['Geo_lat']), np.radians(row['Geo_lng'])]], return_distance=True)\n",
    "    closest_points = gdf_known.iloc[index[0]]\n",
    "    # Filter by the same date\n",
    "    closest_point_same_date = closest_points[closest_points['Date'] == row['Date']]\n",
    "    \n",
    "    if not closest_point_same_date.empty:\n",
    "        # If there are points with the same date, use the closest one\n",
    "        return closest_point_same_date.iloc[0]['Snowfall']\n",
    "    else:\n",
    "        # If there are no points with the same date, use the closest one regardless of the date\n",
    "        return closest_points.iloc[0]['Snowfall']\n",
    "\n",
    "# Apply the function to fill NA values\n",
    "gdf_na['Snowfall'] = gdf_na.apply(fill_snowfall, axis=1)\n",
    "\n",
    "# Update the original DataFrame\n",
    "df.update(gdf_na)\n",
    "\n",
    "df.to_csv('df_with_snowdata.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37eb4a63ed8dadfc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge snowfall data\n",
    "\n",
    "snowdf = pd.read_csv('df_with_snowdata.csv')\n",
    "df = pd.read_csv('utah_traffic_accidents.csv')\n",
    "\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'])\n",
    "df['Date'] = df['Start_Time'].dt.strftime('%Y-%m-%d')  # Create a temporary 'Date' column\n",
    "df['Snowfall'] = pd.NA  # Initialize the Snowfall column\n",
    "\n",
    "\n",
    "df = pd.merge(df, snowdf[['Geo_lng', 'Geo_lat', 'Date', 'Snowfall']], \n",
    "              on=['Geo_lng', 'Geo_lat', 'Date'], \n",
    "              how='left', \n",
    "              suffixes=('', '_new'))\n",
    "\n",
    "# If the original Snowfall column in 'df' has NA values, update them with the values from 'snowfalldf'\n",
    "df['Snowfall'] = df['Snowfall'].combine_first(df['Snowfall_new'])\n",
    "\n",
    "# Drop the temporary 'Snowfall_new' column after the merge\n",
    "df.drop(columns=['Snowfall_new', 'Date'], inplace=True)\n",
    "\n",
    "# save\n",
    "df.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1958a81f27f119"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
