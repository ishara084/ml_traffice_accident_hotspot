{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8529bec2-667e-4408-b86a-6afb74773e6a",
   "metadata": {},
   "source": [
    "Module - CIS7017 Dissertation\n",
    "Student ID - #20275320"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b6650214d0ad84"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f86eec08bab5bb9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T00:14:33.271474900Z",
     "start_time": "2024-04-18T00:14:33.076842200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import all relevant libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('C:/dataset/US_Accidents.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T00:15:24.052290Z",
     "start_time": "2024-04-18T00:14:35.027740Z"
    }
   },
   "id": "1ab7024128d395b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c18ecca3d6ae9c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value Counts for Wind_Direction:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWind_Direction\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mValue Counts for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mdata\u001B[49m[column]\u001B[38;5;241m.\u001B[39mvalue_counts())\n\u001B[0;32m      6\u001B[0m utah_data \u001B[38;5;241m=\u001B[39m data[data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mState\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUT\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Filter for rows where the State column is 'UT' for Utah\n",
    "for column in ['State']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(data[column].value_counts())\n",
    "    \n",
    "utah_data = data[data['State'] == 'UT']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T21:01:55.715231100Z",
     "start_time": "2024-04-17T21:01:55.307598700Z"
    }
   },
   "id": "a502514202324080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Altitude data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a40ed7d64dfc6c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to get elevations for a list of latitudes and longitudes\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Validate latitudes and longitudes\n",
    "    valid_latitudes = [str(lat) for lat in latitudes if -90 <= lat <= 90]\n",
    "    valid_longitudes = [str(lon) for lon in longitudes if -180 <= lon <= 180]\n",
    "    \n",
    "    # Ensure we have the same number of valid latitudes and longitudes\n",
    "    if len(valid_latitudes) != len(valid_longitudes) or not valid_latitudes:\n",
    "        return [None] * len(latitudes)  # Return None for invalid pairs\n",
    "    \n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(valid_latitudes)}&longitude={','.join(valid_longitudes)}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # an exception for HTTP error codes\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Splitting the DataFrame into chunks of 100 rows to comply with the API's limitation\n",
    "chunk_size = 100\n",
    "altitude_list = []\n",
    "\n",
    "# Wrap the range function with tqdm to see the progress\n",
    "for start in tqdm(range(0, utah_data.shape[0], chunk_size), desc='Fetching Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = utah_data.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    altitude_list.extend(elevations)\n",
    "\n",
    "# Add the altitude information to the DataFrame\n",
    "utah_data['Altitude'] = altitude_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a1fb08bba2ef9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run again for failed API requests. \n",
    "# TODO:: Merge both snippets\n",
    "\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Construct the API URL with the given latitudes and longitudes\n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(map(str, latitudes))}&longitude={','.join(map(str, longitudes))}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an error for bad responses\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Filter the DataFrame to rows where Altitude is missing (NaN)\n",
    "missing_altitude_df = utah_data[pd.isna(utah_data['Altitude'])]\n",
    "\n",
    "# Initialize an empty list to store the fetched altitudes\n",
    "fetched_altitudes = []\n",
    "\n",
    "for start in tqdm(range(0, missing_altitude_df.shape[0], chunk_size), desc='Filling Missing Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = missing_altitude_df.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    fetched_altitudes.extend(elevations)\n",
    "\n",
    "# Update the original DataFrame with the newly fetched altitudes\n",
    "for (index, altitude), (_, row) in zip(enumerate(fetched_altitudes), missing_altitude_df.iterrows()):\n",
    "    if altitude is not None:  # Only update if the API call was successful\n",
    "        utah_data.at[row.name, 'Altitude'] = altitude\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae72c1a7a0a0e75d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the draft dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b702c89b5d29bc15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877d2f04d5749c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO::Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c8c54e34d9c2fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb4c3fc3cc7bb424"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "utah_data = pd.read_csv('utah_traffic_accidents.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T00:15:29.621121200Z",
     "start_time": "2024-04-18T00:15:29.417990400Z"
    }
   },
   "id": "f2c2322c5c03d301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utah_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e564a6511dd1bd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Check for missing values \\n\")\n",
    "print(utah_data.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49fbd44b7636735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get value counts for a column\n",
    "for column in ['Precipitation(in)']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(utah_data[column].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf70764501b776cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping columns with less percentage of data and unnecessary columns\n",
    "utah_data = utah_data.drop(columns=['Source', 'End_Lat','End_Lng','Wind_Chill(F)', 'Description', 'Street', 'County', 'Zipcode', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Amenity', 'Bump', 'Give_Way', 'No_Exit', 'Roundabout', 'Traffic_Calming', 'Turning_Loop'],axis=1)\n",
    "utah_data.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b84d00c23188ee27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove data points with missing values (for insignificant amounts)\n",
    "utah_data = utah_data.dropna(subset=['Nautical_Twilight', 'Precipitation(in)'])\n",
    "utah_data=utah_data.dropna(axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e5a31cf14086b45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "utah_data = utah_data.rename(columns={'Start_Lat': 'Geo_lat', 'Start_Lng': 'Geo_lng', 'Distance(mi)': 'Distance', 'Temperature(F)': 'Temperature',\n",
    "                                      'Humidity(%)': 'Humidity', 'Pressure(in)': 'Pressure', 'Visibility(mi)': 'Visibility', 'Wind_Speed(mph)': 'Wind_Speed',\n",
    "                                      'Precipitation(in)': 'Precipitation'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7189c7816cedb59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utah_data = utah_data.drop(columns=['State'],axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe5ba2ce13501f5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Drop NA columns\n",
    "# nan_columns = utah_data.columns[utah_data.isnull().any()].tolist()\n",
    "# if nan_columns:\n",
    "#     raise ValueError(f\"NaN values found in columns: {nan_columns}\")\n",
    "utah_data = utah_data.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T21:28:52.655930500Z",
     "start_time": "2024-04-17T21:28:52.630448Z"
    }
   },
   "id": "78f874fa5c367958"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Calculate 'Time_Duration' in seconds\n",
    "utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time'])\n",
    "utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time'])\n",
    "utah_data['Time_Duration'] = (utah_data['End_Time'] - utah_data['Start_Time']).dt.total_seconds()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T00:15:34.070554100Z",
     "start_time": "2024-04-18T00:15:33.961972700Z"
    }
   },
   "id": "18ee206fd53f2d3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Fixing fractual seconds in Time columns.\n",
    "\n",
    "try:\n",
    "    utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time']).dt.floor('S')\n",
    "    utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time']).dt.floor('S')\n",
    "except Exception as e:\n",
    "    print(\"Error converting dates:\", e)\n",
    "    # Attempt to convert with coercion to find problematic entries\n",
    "    utah_data['Start_Time'] = pd.to_datetime(utah_data['Start_Time'], errors='coerce').dt.floor('S')\n",
    "    utah_data['End_Time'] = pd.to_datetime(utah_data['End_Time'], errors='coerce').dt.floor('S')\n",
    "    # Check for NaT values which indicate failed conversions\n",
    "    problematic_starts = utah_data[utah_data['Start_Time'].isna()]\n",
    "    problematic_ends = utah_data[utah_data['End_Time'].isna()]\n",
    "    if not problematic_starts.empty or not problematic_ends.empty:\n",
    "        print(\"Problematic Start Times:\", problematic_starts)\n",
    "        print(\"Problematic End Times:\", problematic_ends)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c0fd77fd4f5ef7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Re-saving the cleaned dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-18T00:15:40.166437600Z",
     "start_time": "2024-04-18T00:15:39.500784Z"
    }
   },
   "id": "b54ae409f236b7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ## Tests...\n",
    "# #can not be used due to the API limits\n",
    "# \n",
    "# API_KEY = 'hC05ajfrO8WgNBrvZv6j7ifPew7vG2nn'\n",
    "# \n",
    "# BASE_URL = 'https://api.tomtom.com/traffic/services/4/flowSegmentData/absolute/10/json'\n",
    "# COORDINATES = '40.781876,-111.910858'  # Replace with your specific coordinates\n",
    "# \n",
    "# def get_traffic_data(api_key, coordinates):\n",
    "#     params = {\n",
    "#         'key': api_key,\n",
    "#         'point': coordinates,\n",
    "#         'unit': 'KMPH',\n",
    "#     }\n",
    "# \n",
    "#     response = requests.get(BASE_URL, params=params)\n",
    "# \n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()  # Parse JSON response if the call was successful\n",
    "#     else:\n",
    "#         raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "# \n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         traffic_data = get_traffic_data(API_KEY, COORDINATES)\n",
    "#         print(traffic_data)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8bade675b2c995"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
