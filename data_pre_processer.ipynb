{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8529bec2-667e-4408-b86a-6afb74773e6a",
   "metadata": {},
   "source": [
    "Module - CIS7017 Dissertation\n",
    "Student ID - #20275320"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49b6650214d0ad84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86eec08bab5bb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import all relevant libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('C:/dataset/US_Accidents.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab7024128d395b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3c18ecca3d6ae9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for rows where the State column is 'UT' for Utah\n",
    "for column in ['State']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(data[column].value_counts())\n",
    "    \n",
    "utah_data = data[data['State'] == 'UT']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a502514202324080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Altitude data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a40ed7d64dfc6c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to get elevations for a list of latitudes and longitudes\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Validate latitudes and longitudes\n",
    "    valid_latitudes = [str(lat) for lat in latitudes if -90 <= lat <= 90]\n",
    "    valid_longitudes = [str(lon) for lon in longitudes if -180 <= lon <= 180]\n",
    "    \n",
    "    # Ensure we have the same number of valid latitudes and longitudes\n",
    "    if len(valid_latitudes) != len(valid_longitudes) or not valid_latitudes:\n",
    "        return [None] * len(latitudes)  # Return None for invalid pairs\n",
    "    \n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(valid_latitudes)}&longitude={','.join(valid_longitudes)}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # an exception for HTTP error codes\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Splitting the DataFrame into chunks of 100 rows to comply with the API's limitation\n",
    "chunk_size = 100\n",
    "altitude_list = []\n",
    "\n",
    "# Wrap the range function with tqdm to see the progress\n",
    "for start in tqdm(range(0, utah_data.shape[0], chunk_size), desc='Fetching Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = utah_data.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    altitude_list.extend(elevations)\n",
    "\n",
    "# Add the altitude information to the DataFrame\n",
    "utah_data['Altitude'] = altitude_list"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a1fb08bba2ef9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run again for failed API requests. \n",
    "# TODO:: Merge both snippets\n",
    "\n",
    "def get_elevations(latitudes, longitudes):\n",
    "    # Construct the API URL with the given latitudes and longitudes\n",
    "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={','.join(map(str, latitudes))}&longitude={','.join(map(str, longitudes))}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an error for bad responses\n",
    "        return response.json().get('elevation', [None] * len(latitudes))\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API request failed: {e}\")\n",
    "        return [None] * len(latitudes)  # Return None for failed requests\n",
    "\n",
    "# Filter the DataFrame to rows where Altitude is missing (NaN)\n",
    "missing_altitude_df = utah_data[pd.isna(utah_data['Altitude'])]\n",
    "\n",
    "# Initialize an empty list to store the fetched altitudes\n",
    "fetched_altitudes = []\n",
    "\n",
    "for start in tqdm(range(0, missing_altitude_df.shape[0], chunk_size), desc='Filling Missing Altitudes'):\n",
    "    end = start + chunk_size\n",
    "    batch = missing_altitude_df.iloc[start:end]\n",
    "    latitudes = batch['Start_Lat'].tolist()\n",
    "    longitudes = batch['Start_Lng'].tolist()\n",
    "    \n",
    "    elevations = get_elevations(latitudes, longitudes)\n",
    "    fetched_altitudes.extend(elevations)\n",
    "\n",
    "# Update the original DataFrame with the newly fetched altitudes\n",
    "for (index, altitude), (_, row) in zip(enumerate(fetched_altitudes), missing_altitude_df.iterrows()):\n",
    "    if altitude is not None:  # Only update if the API call was successful\n",
    "        utah_data.at[row.name, 'Altitude'] = altitude\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae72c1a7a0a0e75d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the draft dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b702c89b5d29bc15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "877d2f04d5749c79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO::Integrate Temperature Variations, Oxygen Levels, UV Radiation, Hazards etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c8c54e34d9c2fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb4c3fc3cc7bb424"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the saved dataset\n",
    "utah_data = pd.read_csv('utah_traffic_accidents.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c2322c5c03d301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utah_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e564a6511dd1bd6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Check for missing values \\n\")\n",
    "print(utah_data.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49fbd44b7636735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get value counts for a column\n",
    "for column in ['Precipitation(in)']:\n",
    "    print(f\"\\nValue Counts for {column}:\")\n",
    "    print(utah_data[column].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf70764501b776cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping columns with less percentage of data and unnecessary columns\n",
    "utah_data = utah_data.drop(columns=['End_Lat','End_Lng','Wind_Chill(F)', 'Description', 'Street', 'County', 'Zipcode', 'Timezone', 'Airport_Code', 'Weather_Timestamp', 'Amenity', 'Bump', 'Give_Way', 'No_Exit', 'Roundabout', 'Traffic_Calming', 'Turning_Loop'],axis=1)\n",
    "utah_data.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b84d00c23188ee27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove data points with missing values (for insignificant amounts)\n",
    "utah_data = utah_data.dropna(subset=['Nautical_Twilight', 'Precipitation(in)'])\n",
    "utah_data=utah_data.dropna(axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e5a31cf14086b45"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Re-saving the cleaned dataset to a CSV file\n",
    "utah_data.to_csv('utah_traffic_accidents.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b54ae409f236b7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
